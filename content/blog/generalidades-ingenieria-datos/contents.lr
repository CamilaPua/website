title: Generalidades de Ingenieria de Datos
---
author: andres-perez
---
body:

El mundo de la ciencia de datos es enorme e imposible de aprender en su totalidad, pero todo empieza por nociones. Espero que este artículo te funcione como una breve referencia a todos esos conceptos base. Si funciona como una vista general y básica de la ciencia de datos, ha cumplido su cometido. Pronto tendremos la charla [Modelado de una DataWarehouse con DBT](https://www.meetup.com/pythonbaq/events/294769421/) en PyBAQ que tratará un tema muy específico de este mundo.

### Estadística en Ciencia de Datos

Si tienes interés en Python en estos días escucharás muchísimo de Análisis, Ciencia, Ingeniería o Arquitectura de Datos (entre otros términos o disciplinas). Cada uno de los anteriores términos representa o describe diferentes trayectorias profesionales que podemos tomar en el mundo de los datos. Pero el inicio de la ciencia de datos es más sencillo que todos estos términos; todo empieza con estadísticas[^1].

![Pequeña ilustración de Estadística: Regresión Lineal](https://upload.wikimedia.org/wikipedia/commons/thumb/0/02/KorrRes.png/302px-KorrRes.png "Pequeña ilustración de Estadística: Regresión Lineal")

La estadística no solo es el inicio sino también la columna vertebral de la ciencia de datos. Como tal, incorpora habilidades de ciencias de computación, estadística, ciencias de información, matemáticas, visualización de datos, visualización de información, sonificación de datos, integración de datos, diseño grafico, sistemas complejos, comunicación y negocios. Nos sirve pensar esto porque la estadística exige o requiere un mínimo de los datos que estudia[^2]: que los datos estén completos, que sean consistentes, que sean preferentemente precisos y también homogéneos con respecto a lo que describen o miden. Se requiere un esfuerzo significativo para establecer sistemas que de manera consistente produzcan datos de tal calidad.



<img src="/img/posts/2023/generalidades-ingenieria-datos/image1.png" alt="Las estadisticas maquilladas son Ciencia de Datos" title="Las estadisticas maquilladas son Ciencia de Datos" style="zoom:100%;" />

### Origen de Datos, Tipos de Datos y Lagos

La información no se genera según un cronograma en la vida real. Las entregas se retrasan, también lo hacen los procesos de fabricación, sean automatizados o no. La vida sucede cuando no la esperamos y a veces no sucede cuando estamos acostumbrados; dejemos esa idea en remojo, solo retengamos que el flujo de datos no siempre es predecible. Por ese motivo se creó la noción de Lagos de Datos. Los lagos de datos ayudan a almacenar la información que se ha considerado necesario analizar, en ellos no se procesan datos salvo para catalogar entre los tipos de datos y asegurar que la información del origen no se pierda o altere antes de ser analizada. El lago de datos idealmente retiene información del origen de datos y el tipo de dato almacenado, sean estructurados, semi-estructurados o no estructurados. 

Los datos estructurados son datos que tienen un formato estandarizado para acceso eficiente tanto por software como por humanos[^3]. Típicamente el formato es tabular con filas y columnas que claramente definen atributos de datos. Las computadoras pueden procesar efectivamente datos estructurados. El documento que garantiza o documenta la estructura de datos se llama “schema” (que traduce esquema, o esquema de datos en este contexto). En SQL (por sus siglas en inglés Structured Query Language; en español lenguaje de consulta estructurada) existen una serie de comandos orientados al modelado de datos. Las databases resultado del modelo de datos que se cree a través de SQL garantizará que los datos que se inserten cumplan los requerimientos del modelo. Lo importante a retener: Existen datos estructurados y los schemas garantizan/salvaguardan su estructura.

Los datos semi-estructurados no se adhieren a un modelo de datos específico, pero tienen elementos estructurados bien sea en el formato o que algunos atributos correspondan a un modelo mientras otros no[^4]. XML y JSON son los ejemplos clásicos de este tipo de datos porque tienen una estructura estandarizada que, sin embargo, no garantiza una la estructura de los datos que almacena. Por su flexibilidad estos formatos han sido usados en aplicaciones de escritorio y web por décadas; por eso existen herramientas para forzar/verificar que estos mantengan una estructura definida. Para XML podemos mostrar los XSD y para JSON existe json-schema. Este tipo de schemas permiten estructurar parcialmente los datos guardados en estos formatos por lo que la noción de datos semi-estructurados se mantiene aunque exista un schema.

Los datos no estructurados superan a todos los anteriores en volumen y su estructura es impredecible[^5]. Un ejemplo cercano a los datos semi-estructurados son las páginas web. Estas contienen texto, imágenes, videos, formularios y funcionalidades que son datos no estructurados; son no estructurados porque la estructura no captura el significado del contenido. Otro ejemplo importante de datos no estructurados son los registros médicos: tanto los registros escritos como aquellos derivados de la lectura de imágenes diagnósticas contienen muchísima información basada en el contexto de la relación médico-paciente y los síntomas siendo analizados en el momento. Una gran cantidad del desarrollo de inteligencias artificiales tiene como meta lograr interpretar estos datos con el fin de facilitar su análisis.

¿Que es importante retener de esta sección? Existen varios tipos de datos y se puede usar un lago de datos para almacenar muchos tipos de datos. Agreguemos una noción: el lago de datos puede ser algo que cree y mantenga una organización o puede ser contratado como un servicio de terceros.

### Una breve alegoría de Ingeniería de Datos

Empezamos con el concepto de lago de datos (Data Lake) y debemos seguir con el Almacén de datos (Data Warehouse) pero a veces el lenguaje simbólico se puede mantener a fin de crear una alegoría. Entonces pensemos en el almacén de datos como un tanque elevado. El lago (de datos) es de donde sacamos el agua (datos) que llegará al tanque elevado (data warehouse). Para potabilizar el agua hay que extraerla del lago, procesarla y luego transmitirla al lugar de almacenamiento, este proceso se llama ETL (“Extraction, Transformation and Loading” de sus siglas en inglés, traduce Extracción, Transformación y Carga). El agua se mueve a través de tubería y de allí se obtuvo la simbología de “Tubería de Datos” (Data Pipelines). La alegoría se rompe en el aspecto de tubería de datos, ya que a diferencia del proceso de tratamiento de aguas, los datos se procesan de manera diferente según su tipo y características. Se mantiene en el sentido que al final del proceso todos estos datos deben terminar en una estructura pensada para facilitar su uso y análisis, lo que es equivalente al consumo de agua potable.



<img src="/img/posts/2023/generalidades-ingenieria-datos/image2.png" alt="Alegoría de Ingenieria de Datos como Tratamiento de Agua Potable" title="Alegoría de Ingeniería de Datos como Tratamiento de Agua Potable" />

### Modelos de datos y sus tipos

Los modelos de datos de aplicaciones por lo general están creadas usando databases relacionales, esto es, tablas con filas y columnas que representan instancias y atributos respectivamente. A fin de tener datos correctos y fáciles de mantener, en general se normalizan hasta la 3ra forma normal. Estos modelos son conocidos como OLTP porque, por su naturaleza se implementan para procesos transaccionales en línea (por ejemplo, cuando compras un jeans en una tienda de retail o cuando pagas en el super con tu TC). Una database modelada y configurada de esta manera se considera óptima para OLTP (Online Transaction Processing de sus siglas en inglés, Procesamiento de Transacciones en Línea). Las databases OLTP son óptimas para las operaciones UPDATE, INSERT, DELETE. Si quisiéramos hacer reportes basados en una database optimizada para OLTP de seguro tendríamos que usar consultas de SQL que muchas operaciones de unión, lo que penaliza la operación de la database y hace las consultas difíciles de mantener en el tiempo.

La contraparte de un modelo de datos OLTP sería crear un modelo de datos que no esté normalizado hasta su tercera forma normal y que facilite la consulta. Este sería un modelo optimizado para el análisis de datos y se llama OLAP (Online Analytical Processing de sus siglas en inglés, Procesamiento Analítico en Línea)[^6]. En este aspecto el modelo de estrella (Star Schema) y el de copo de nieve (Snowflake Schema) son los más utilizados. Para migrar datos de un modelo OLTP a otro correspondiente OLAP se debe entender el modelo de origen y entender cómo crear el modelo de destino. Acerca de un modelo de datos de estrella debemos saber que hay 2 tipos de tablas: tablas de hechos y tablas dimensionales[^7]. Las tablas de hechos describen un hecho concreto: la venta de un ítem, la llegada de un producto o la terminación de un proceso. Las tablas dimensionales representan datos extras o dimensiones acerca de ese hecho; una dimensión de fecha registra en qué fecha sucedió el hecho, por ejemplo. En un modelo de copo de nieve (snowflake) las dimensiones pueden subdividirse en subdimensiones; fecha y hora son un ejemplo de posibles subdimensiones de una dimensión de tiempo.

| <img src="/img/posts/2023/generalidades-ingenieria-datos/image3.png" alt="alt_text" title="image_tooltip" style="zoom:50%;" /> | <img src="/img/posts/2023/generalidades-ingenieria-datos/image4.png" alt="alt_text" title="image_tooltip" style="zoom:50%;" /> |
| :----------------------------------------------------------: | :----------------------------------------------------------: |


Desde una perspectiva de gestión de proyectos, estos procesos pueden implementarse de múltiples maneras. En la historia de arquitectura de datos se consideran los métodos Kimball e Inmon[^8] como las bases teóricas más conocidas. Estos se mencionan para que puedan expandir en el tema por su parte.

A manera de resumen, un almacén de datos es un repositorio planeado y estructurado. Se usa un esquema OLAP, por lo general un modelo de estrella o de copo de nieve. Usando ese modelo se almacenan datos desnormalizados y estos a su vez están disponibles para ser analizados, formar parte de reportes, ser transformados aún más o todas las anteriores.

### Resultados y visualización

Al final de todo este proceso y sin importar cuantas disciplinas diferentes hayan estado involucradas, lo único visible de todo este esfuerzo es lo que los usuarios finales puedan usar. En este respecto existen los reportes y los tableros de control (dashboards). En este respecto no hay mucho que decir salvo que no deben ser subestimados. Existen cientos de herramientas diseñadas para facilitar la creación de reportes y tableros[^9].

Los tableros sólo pueden reflejar la calidad de datos generados en los procesos anteriores y son la razón detrás de la necesidad de revisitar y modificar los procesos de transformación una y otra vez. Bien sea el usuario final o los analistas de datos necesitan poder hacer inferencias de los datos y presentar las conclusiones alcanzadas de una manera efectiva. Existen gráficos diseñados y perfeccionados en el tiempo para representar muchas características estadísticas de los datos. Existen otros gráficos necesarios para contextualizar rápidamente la información, siendo un ejemplo claro la utilización de mapas en datos geolocalizados. Abajo se muestra un ejemplo de tablero generado usando Apache Superset.


![Dashboard con Apache Superset](/img/posts/2023/generalidades-ingenieria-datos/image5.png "Dashboard con Apache Superset")

### Por Aprender

Como ven el mundo de la ciencia de datos e ingeniería de datos no empezó ayer ni se terminará mañana. Para tener una visión general de todos los frameworks pagos y no pagos relacionados en estas disciplinas les recomiendo ir al siguiente link: [Interactive MAD Landscape by Matt Turck](https://mad.firstmark.com/). Quien nos deja un tweet que muy elegantemente refleja la complejidad de la tarea que tienen los ingenieros de datos hoy:

<blockquote class="twitter-tweet"><p lang="en" dir="ltr">It’s easy to succeed in data/AI. All you need is to put data into a DWH for OLAP using ETL, or ELT with DBT. Spot any issues on your DAG, run SQL for some BI, reverse ETL into SaaS or use a CDP, then leverage for ML/AI with CNNs, RNNs, RLs, GANs, and maybe LLMs or LDMs for GenAI.</p>&mdash; Matt Turck (@mattturck) <a href="https://twitter.com/mattturck/status/1610765711529414660?ref_src=twsrc%5Etfw">January 4, 2023</a></blockquote> <script async src="https://platform.twitter.com/widgets.js" charset="utf-8"></script>

Por favor inscríbanse al meetup la próxima semana (encuentran el vincula en el primer párrafo) y estudien. Seguramente si hay interesados pueda hacerse un grupo de estudio de ingeniería de datos. También, este post es interesante como punto de partida en el futuro para aquellos interesados en empezar esta carrera profesional, siéntanse en libertad de sugerir fuentes o cambios el repositorio de [github de PyBAQ](https://github.com/PyBAQ/website).

Incluyo algunas librerias relevantes en el tema de ingenieria de datos:
* [PETL](https://github.com/petl-developers/petl)
* [Airbyte](https://github.com/airbytehq/airbyte)
* [dbt Labs](https://www.getdbt.com/)
* [Apache Airflow](https://airflow.apache.org/)
* [Spotify Luigi ](https://github.com/spotify/luigi)
* [Apache Superset](https://superset.apache.org/)
* [Plotly Python Graphing Library](https://plotly.com/python/)

Un agradecimiento especial a [Javier Daza](/nosotros/javier-daza/), Marcelo Herrera con sus comentarios acerca del artículo y sobre todo a Jorge Aguilar por el contexto inicial brindado en referencia a su próxima [Modelado de una DataWarehouse con DBT](https://www.meetup.com/pythonbaq/events/294769421/) en PyBAQ.

[^1]: [Data science - Wikipedia](https://en.wikipedia.org/wiki/Data_science#cite_note-:7-20), [50YearsDataScience.pdf (mit.edu)](http://courses.csail.mit.edu/18.337/2015/docs/50YearsDataScience.pdf)

[^2]: [Reliability (statistics) - Wikipedia](https://en.wikipedia.org/wiki/Reliability_(statistics))
[^3]: [What is Structured Data? - Structured Data Explained - AWS (amazon.com)](https://aws.amazon.com/what-is/structured-data/#:~:text=Structured%20data%20is%20data%20that,due%20to%20its%20quantitative%20nature.)
[^4]: [What is Semi-structured data? - GeeksforGeeks](https://www.geeksforgeeks.org/what-is-semi-structured-data/)
[^5]: [Structured Data vs. Unstructured Data - What's The Difference? - Treehouse Tech Group](https://treehousetechgroup.com/structured-data-vs-unstructured-data-whats-the-difference/), [8 Examples of Unstructured Data - Treehouse Tech Group](https://treehousetechgroup.com/8-examples-of-unstructured-data/)
[^6]: [OLAP vs. OLTP: What’s the Difference? | IBM](https://www.ibm.com/cloud/blog/olap-vs-oltp)
[^7]: [Differences Between Fact Table and Dimension Table | Simplilearn](https://www.simplilearn.com/fact-table-vs-dimension-table-article#:~:text=Fact%20tables%20have%20more%20records,table%20has%20a%20concatenated%20key.)
[^8]: [Inmon vs Kimball - the great data warehousing debate | by INDRAJITH EKANAYAKE | CloudZone | Medium](https://medium.com/cloudzone/inmon-vs-kimball-the-great-data-warehousing-debate-78c57f0b5e0e), [Kimball DW/BI Lifecycle Methodology - Kimball Group](https://www.kimballgroup.com/data-warehouse-business-intelligence-resources/kimball-techniques/dw-bi-lifecycle-method/)
[^9]:[Choosing an Analytics Tool. Metabase Vs Superset Vs Redash | by Stefan Mihaylov | VorTECHsa | Medium](https://medium.com/vortechsa/choosing-an-analytics-tool-metabase-vs-superset-vs-redash-afd88e028ba9)
---
excerpt: El mundo de la ciencia de datos es enorme e imposible de aprender en su totalidad, pero todo empieza por nociones. Espero que este artículo te funcione como una breve referencia a todos esos conceptos base. Si funciona como una vista general y básica de la ciencia de datos, ha cumplido su cometido. Pronto tendremos la charla [Modelado de una DataWarehouse con DBT](https://www.meetup.com/pythonbaq/events/294769421/) en PyBAQ que tratará un tema muy específico de este mundo.
---
pub_date: 2023-07-21
